{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a588d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from llama_index.core.prompts import RichPromptTemplate\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.llms import ChatMessage, TextBlock, ImageBlock\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import logging\n",
    "from llama_index.core.callbacks import LlamaDebugHandler, CallbackManager\n",
    "from llama_index.core.agent.workflow import ReActAgent,FunctionAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.settings import Settings\n",
    "from pathlib import Path\n",
    "from git import Repo\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    set_global_handler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ca4db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        ID              SIZE      MODIFIED     \n",
      "qwen3:8b                    500a1f067a9f    5.2 GB    28 hours ago    \n",
      "qwen3:14b                   bdbd181c33f2    9.3 GB    28 hours ago    \n",
      "deepseek-r1:latest          6995872bfe4c    5.2 GB    47 hours ago    \n",
      "gemma3:4b                   a2af6cc3eb7f    3.3 GB    6 days ago      \n",
      "mxbai-embed-large:latest    468836162de7    669 MB    9 days ago      \n",
      "gemma3:12b                  f4031aab637d    8.1 GB    2 weeks ago     \n",
      "nomic-embed-text:latest     0a109f422b47    274 MB    3 weeks ago     \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e72b7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_provider=\"Ollama (Local)\"\n",
    "thinking=False\n",
    "model_name=\"gemma3:12b\"\n",
    "temperature=0.1\n",
    "context_window=8096\n",
    "max_tokens=context_window\n",
    "\n",
    "def get_llm():\n",
    "    if model_provider == \"Ollama (Local)\":\n",
    "        if thinking:\n",
    "            return Ollama(\n",
    "                model=model_name,\n",
    "                temperature=temperature,\n",
    "                context_window=context_window,\n",
    "                request_timeout=500,\n",
    "                thinking=thinking\n",
    "            )\n",
    "        else:\n",
    "            return Ollama(\n",
    "                model=model_name,\n",
    "                temperature=temperature,\n",
    "                context_window=context_window,\n",
    "                request_timeout=500\n",
    "            )\n",
    "    else:\n",
    "        return OpenAI(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            request_timeout=500\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e201eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollamaEmbeddings():\n",
    "    embed_model=OllamaEmbedding(\n",
    "    model_name=\"mxbai-embed-large:latest\",\n",
    "    embed_batch_size=32\n",
    "    )\n",
    "    return embed_model\n",
    "\n",
    "\n",
    "def set_ollama():\n",
    "    llm=get_llm()\n",
    "    embed_model=get_ollamaEmbeddings()\n",
    "    Settings.llm=llm\n",
    "    Settings.embed_model=embed_model\n",
    "    \n",
    "\n",
    "# Initialize or load index\n",
    "\n",
    "def _load_init_index(repo_path):\n",
    "    \"\"\"Load existing index\"\"\"\n",
    "    index_path=repo_path+'/.code_index'\n",
    "    try:\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=str(index_path))\n",
    "        index = load_index_from_storage(storage_context)\n",
    "    except:\n",
    "    # Load all Python files\n",
    "        documents = SimpleDirectoryReader(\n",
    "            input_dir=str(repo_path),\n",
    "            required_exts=[\".py\"],\n",
    "            recursive=True\n",
    "        ).load_data()\n",
    "        \n",
    "        # Create and save index\n",
    "        index = VectorStoreIndex.from_documents(documents)\n",
    "        index.storage_context.persist(persist_dir=str(index_path))\n",
    "    finally:\n",
    "        return index\n",
    "\n",
    "def _create_agent():\n",
    "    \"\"\"Create a ReAct agent with custom tools\"\"\"\n",
    "    # Define tools\n",
    "    check_changes_tool = FunctionTool.from_defaults(fn=check_code_changes)\n",
    "    generate_test_cases_tool = FunctionTool.from_defaults(fn=generate_test_cases)\n",
    "    update_index_tool = FunctionTool.from_defaults(fn=_load_init_index)\n",
    "    generate_tests_tool = FunctionTool.from_defaults(fn=generate_tests)\n",
    "    \n",
    "    agent = FunctionAgent(\n",
    "        tools=[check_changes_tool,generate_test_cases_tool,generate_tests_tool,update_index_tool],\n",
    "        llm=get_llm(),\n",
    "        verbose=True,\n",
    "        CallbackManager=CallbackManager\n",
    "    )\n",
    "\n",
    "    return agent\n",
    "\n",
    "def check_code_changes(repo_path:str,since_commit: str = \"HEAD~1\",agentic=True) -> str:\n",
    "    \"\"\"\n",
    "    Check what Python files have changed since a specific commit.\n",
    "    \n",
    "    Args:\n",
    "        since_commit: The commit to compare against (default: previous commit)\n",
    "        \n",
    "    Returns:\n",
    "        String describing the changed files and their diffs\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        repo = Repo(repo_path)\n",
    "        print(f\"Repository: {repo_path}\")\n",
    "        print(f\"Comparing: {since_commit} ({repo.commit(since_commit).hexsha[:7]}) to HEAD ({repo.head.commit.hexsha[:7]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening repository at {repo_path}: {str(e)}\")\n",
    "\n",
    "    # Get changed files\n",
    "    changed_files = []\n",
    "    diffs = repo.commit(since_commit).diff(repo.head.commit)\n",
    "    if not diffs:\n",
    "        print(\"No differences found between the commits\")\n",
    "    else:\n",
    "        for diff in diffs:\n",
    "            file_path = getattr(diff, \"a_path\", None) or getattr(diff, \"b_path\", None)\n",
    "            # Some diffs may not have a diff attribute (e.g., binary files or renames)\n",
    "            diff_content = \"\"\n",
    "            if hasattr(diff, \"diff\") and diff.diff:\n",
    "                try:\n",
    "                    diff_content = diff.diff.decode(\"utf-8\", errors=\"replace\")\n",
    "                except Exception:\n",
    "                    diff_content = \"<Could not decode diff>\"\n",
    "            print(f\"Found changes in {file_path} ({getattr(diff, 'change_type', '?')})\")\n",
    "            if diff_content:\n",
    "                print(f\"Diff length: {len(diff_content)} bytes\")\n",
    "            # If diff_content is empty, try to get the diff using repo.git.diff as a fallback\n",
    "            if not diff_content and file_path:\n",
    "                try:\n",
    "                    diff_content = repo.git.diff(f\"{since_commit}..HEAD\", \"--\", file_path)\n",
    "                except Exception:\n",
    "                    diff_content = \"<Could not retrieve diff via git command>\"\n",
    "            changed_files.append({\n",
    "                \"path\": file_path,\n",
    "                \"change_type\": getattr(diff, \"change_type\", \"?\"),\n",
    "                \"diff\": diff_content,\n",
    "            })\n",
    "\n",
    "    if not changed_files:\n",
    "        return \"No Python files have changed since the specified commit.\"\n",
    "        \n",
    "    # Format output\n",
    "    if agentic:\n",
    "        output = [\"Changed Python files:\"]\n",
    "        for file in changed_files:\n",
    "            output.append(f\"\\nFile: {file['path']} ({file['change_type']})\")\n",
    "            output.append(f\"Diff:\\n{file['diff']}\")\n",
    "        return \"\\n\".join(output)\n",
    "    else:\n",
    "        output = []\n",
    "        for file in changed_files:\n",
    "            output.append(f\"{file['path']}\")\n",
    "        return output\n",
    "        \n",
    "\n",
    "def generate_tests( repo_path,file_path: str, framework: str = \"pytest\",streaming=False) -> str:\n",
    "    \"\"\"\n",
    "    Generate tests for a specific Python file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Python file\n",
    "        framework: Testing framework to use (default: pytest)\n",
    "        \n",
    "    Returns:\n",
    "        Generated test code as a string\n",
    "    \"\"\"\n",
    "    # Read the file content\n",
    "    repopath=Path(repo_path)\n",
    "    full_path = repopath / file_path\n",
    "    if not full_path.exists():\n",
    "        return f\"Error: File {file_path} does not exist.\"\n",
    "        \n",
    "    with open(full_path, \"r\") as f:\n",
    "        code_content = f.read()\n",
    "    \n",
    "    # Use LLM to generate tests\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following Python code and generate comprehensive test cases using {framework}.\n",
    "    Focus on edge cases, input validation, and expected behavior.\n",
    "    \n",
    "    Code:\n",
    "    {code_content}\n",
    "    \n",
    "    Please return only the test code with appropriate imports, no additional explanation.\n",
    "    The tests should be production-ready and follow best practices.\n",
    "    \"\"\"\n",
    "    llm=Settings.llm\n",
    "    if streaming:\n",
    "        response=llm.stream_complete(prompt)\n",
    "        return response\n",
    "    else:\n",
    "        response=llm.complete(prompt)\n",
    "        return response\n",
    "\n",
    "\n",
    "def generate_test_cases(index,changes: str,streaming=False) -> str:\n",
    "    \"\"\"\n",
    "    Generate test cases for a specific Python file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Python file\n",
    "        \n",
    "    Returns:\n",
    "        Generated test cases as a markup format\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use LLM to generate tests\n",
    "    prompt = f\"\"\"\n",
    "        You are an expert test engineer who excels in creating test cases for IT products, \n",
    "        Analyze the following Python code and create test cases, Focusing on edge cases, input validation, and expected behavior.\n",
    "        \n",
    "        Code:\n",
    "        {changes}\n",
    "        \n",
    "        The test cases should follow the following markdown format: \n",
    "\"| Test Case ID | TC-PERF-001 |\\n\",\n",
    "\"|--------------|------------|\\n\",\n",
    "\"| **Title** | Verify API response time under load |\\n\",\n",
    "\"| **Description** | Ensure GraphQL API responds within 500ms under expected load |\\n\",\n",
    "\"| **Preconditions** | Load testing environment set up |\\n\",\n",
    "\"| **Test Steps** | 1. Simulate concurrent users accessing API endpoints<br>2. Measure response times |\\n\",\n",
    "\"| **Expected Result** | API response times remain below 500ms under load |\\n\",\n",
    "\"\\n\",\n",
    "\"---\\n\",\n",
    "\"\\n\",\n",
    "\"\"\" \n",
    "    qe=index.as_query_engine(streaming=streaming, similarity_top_k=1)\n",
    "    if streaming:\n",
    "        return qe.query(prompt).response_gen\n",
    "    else:\n",
    "        return qe.query(prompt).response\n",
    "\n",
    "\n",
    "\n",
    "# Configure Chrome options\n",
    "def get_chrome_options():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    return chrome_options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90b8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm=get_llm()\n",
    "Settings.embed_model=get_ollamaEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb90ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repoPath=\"snake/build\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d3ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from snake/build/.code_index/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from snake/build/.code_index/index_store.json.\n"
     ]
    }
   ],
   "source": [
    "index=_load_init_index(repo_path=repoPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49891c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository: snake/build\n",
      "Comparing: HEAD~1 (3f602dd) to HEAD (fca8f74)\n",
      "Found changes in snake.py (M)\n"
     ]
    }
   ],
   "source": [
    "changes=check_code_changes(repoPath,agentic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9628a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Changed Python files:\n",
       "\n",
       "File: snake.py (M)\n",
       "Diff:\n",
       "diff --git a/snake.py b/snake.py\n",
       "index 62f7b09..b801b3b 100644\n",
       "--- a/snake.py\n",
       "+++ b/snake.py\n",
       "@@ -24,7 +24,7 @@ pygame.display.set_caption('Snake Game')\n",
       " clock = pygame.time.Clock()\n",
       " \n",
       " # Snake block size and speed\n",
       "-snake_block = 10\n",
       "+snake_block = 20\n",
       " snake_speed = 15\n",
       " \n",
       " # Font styles"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display,Markdown\n",
    "Markdown(f'{changes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9ccc265",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=generate_tests(repo_path=repoPath,file_path=changes[0],streaming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fff2d47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "dsfsdf\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "ewrewr"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(\"dsfsdf\\n\\n\\n---\\n\\newrewr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d748c169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***Generating unit test for: snake.py***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown,display\n",
    "change=changes[0]\n",
    "Markdown(f\"***Generating unit test for: {change}***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ae15aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1, 2, 3]'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbc331b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import pytest\n",
       "import pygame\n",
       "from pygame import event\n",
       "from pygame import display\n",
       "from pygame import time\n",
       "from pygame import draw\n",
       "from pygame import font\n",
       "import random\n",
       "\n",
       "# Mocking pygame to avoid actual display and dependencies\n",
       "class MockSurface:\n",
       "    def __init__(self, width, height):\n",
       "        self.width = width\n",
       "        self.height = height\n",
       "\n",
       "    def fill(self, color):\n",
       "        pass\n",
       "\n",
       "    def blit(self, value, position):\n",
       "        pass\n",
       "\n",
       "    def get_width(self):\n",
       "        return self.width\n",
       "\n",
       "    def get_height(self):\n",
       "        return self.height\n",
       "\n",
       "pygame.display.set_mode = lambda x, y: MockSurface(x, y)\n",
       "pygame.font.SysFont = lambda font_name, font_size: lambda text, antialias, color: text\n",
       "pygame.draw.rect = lambda surface, color, rect, width: None\n",
       "pygame.init = lambda: None\n",
       "pygame.quit = lambda: None\n",
       "pygame.time.Clock = lambda: None\n",
       "\n",
       "def your_score(score):\n",
       "    \"\"\"Display the current score\"\"\"\n",
       "    value = font.render(\"Your Score: \" + str(score), True, (0, 0, 0))\n",
       "    display.get_surface().blit(value, [0, 0])\n",
       "\n",
       "def our_snake(snake_block, snake_list):\n",
       "    \"\"\"Draw the snake on the screen\"\"\"\n",
       "    for x in snake_list:\n",
       "        draw.rect(display.get_surface(), (0, 255, 0), [x[0], x[1], 35, 35])\n",
       "\n",
       "def message(msg, color):\n",
       "    \"\"\"Display a message on the screen\"\"\"\n",
       "    mesg = font.render(msg, True, color)\n",
       "    display.get_surface().blit(mesg, [display.get_surface().get_width() / 6, display.get_surface().get_height() / 3])\n",
       "\n",
       "def game_loop():\n",
       "    \"\"\"Main game loop\"\"\"\n",
       "    game_over = False\n",
       "    game_close = False\n",
       "\n",
       "    # Initial snake position\n",
       "    x1 = display.get_surface().get_width() / 2\n",
       "    y1 = display.get_surface().get_height() / 2\n",
       "\n",
       "    # Snake movement\n",
       "    x1_change = 0\n",
       "    y1_change = 0\n",
       "\n",
       "    # Snake body\n",
       "    snake_list = []\n",
       "    length_of_snake = 1\n",
       "\n",
       "    # Food position\n",
       "    foodx = round(random.randrange(0, display.get_surface().get_width() - 35) / 10.0) * 10.0\n",
       "    foody = round(random.randrange(0, display.get_surface().get_height() - 35) / 10.0) * 10.0\n",
       "\n",
       "    while not game_over:\n",
       "\n",
       "        while game_close == True:\n",
       "            display.get_surface().fill((255, 255, 255))\n",
       "            message(\"You Lost! Press Q-Quit or C-Play Again\", (213, 50, 80))\n",
       "            your_score(length_of_snake - 1)\n",
       "            display.update()\n",
       "\n",
       "            for event in event.get():\n",
       "                if event.type == pygame.KEYDOWN:\n",
       "                    if event.key == pygame.K_q:\n",
       "                        game_over = True\n",
       "                        game_close = False\n",
       "                    if event.key == pygame.K_c:\n",
       "                        game_loop()\n",
       "\n",
       "        for event in event.get():\n",
       "            if event.type == pygame.QUIT:\n",
       "                game_over = True\n",
       "            if event.type == pygame.KEYDOWN:\n",
       "                if event.key == pygame.K_LEFT:\n",
       "                    x1_change = -35\n",
       "                    y1_change = 0\n",
       "                elif event.key == pygame.K_RIGHT:\n",
       "                    x1_change = 35\n",
       "                    y1_change = 0\n",
       "                elif event.key == pygame.K_UP:\n",
       "                    y1_change = -35\n",
       "                    x1_change = 0\n",
       "                elif event.key == pygame.K_DOWN:\n",
       "                    y1_change = 35\n",
       "                    x1_change = 0\n",
       "\n",
       "        # Check for boundary collision\n",
       "        if x1 >= display.get_surface().get_width() or x1 < 0 or y1 >= display.get_surface().get_height() or y1 < 0:\n",
       "            game_close = True\n",
       "\n",
       "        # Update snake position\n",
       "        x1 += x1_change\n",
       "        y1 += y1_change\n",
       "        display.get_surface().fill((255, 255, 255))\n",
       "        \n",
       "        # Draw food\n",
       "        draw.rect(display.get_surface(), (213, 50, 80), [foodx, foody, 35, 35])\n",
       "        \n",
       "        # Update snake body\n",
       "        snake_head = []\n",
       "        snake_head.append(x1)\n",
       "        snake_head.append(y1)\n",
       "        snake_list.append(snake_head)\n",
       "        \n",
       "        # Remove extra segments if snake hasn't eaten\n",
       "        if len(snake_list) > length_of_snake:\n",
       "            del snake_list[0]\n",
       "\n",
       "        # Check for self collision\n",
       "        for x in snake_list[:-1]:\n",
       "            if x == snake_head:\n",
       "                game_close = True\n",
       "\n",
       "        # Draw snake and score\n",
       "        our_snake(35, snake_list)\n",
       "        your_score(length_of_snake - 1)\n",
       "\n",
       "        display.update()\n",
       "\n",
       "        # Check if snake ate food\n",
       "        if x1 == foodx and y1 == foody:\n",
       "            foodx = round(random.randrange(0, display.get_surface().get_width() - 35) / 10.0) * 10.0\n",
       "            foody = round(random.randrange(0, display.get_surface().get_height() - 35) / 10.0) * 10.0\n",
       "            length_of_snake += 1\n",
       "\n",
       "        time.Clock().tick(15)\n",
       "\n",
       "    pygame.quit()\n",
       "    quit()\n",
       "\n",
       "@pytest.fixture\n",
       "def mock_pygame():\n",
       "    \"\"\"Fixture to mock pygame for testing.\"\"\"\n",
       "    pass\n",
       "\n",
       "def test_your_score(mock_pygame):\n",
       "    \"\"\"Test the your_score function.\"\"\"\n",
       "    assert True  # Placeholder, add actual test logic\n",
       "\n",
       "def test_our_snake(mock_pygame):\n",
       "    \"\"\"Test the our_snake function.\"\"\"\n",
       "    assert True  # Placeholder, add actual test logic\n",
       "\n",
       "def test_message(mock_pygame):\n",
       "    \"\"\"Test the message function.\"\"\"\n",
       "    assert True  # Placeholder, add actual test logic\n",
       "\n",
       "def test_game_loop_boundary_collision(mock_pygame):\n",
       "    \"\"\"Test game loop boundary collision.\"\"\"\n",
       "    assert True  # Placeholder, add actual test logic\n",
       "\n",
       "def test_game_loop_key_presses(mock_pygame):\n",
       "    \"\"\"Test game loop key presses.\"\"\"\n",
       "    assert True  # Placeholder, add actual test logic\n",
       "\n",
       "def test_game_loop_food_consumption(mock_pygame):\n",
       "    \"\"\"Test game loop food consumption.\"\"\"\n",
       "    assert True  # Placeholder, add actual test logic\n",
       "```\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(resp.text+\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4716bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=generate_tests(repo_path=repoPath,file_path=changes[0],streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e7265c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m full_response \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdelta\n\u001b[1;32m      4\u001b[0m Markdown(full_response)\n\u001b[0;32m----> 5\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "full_response=''\n",
    "for response in resp:\n",
    "    full_response += response.delta\n",
    "    Markdown(full_response)\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c490e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown,display\n",
    "full_response = \"\"\n",
    "for txt in resp:\n",
    "    tst=txt.text\n",
    "    #print(tst)\n",
    "    full_response+=tst\n",
    "    display(Markdown(tst))\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "qe=index.as_query_engine(streaming=True, similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c294672",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=qe.query(f\"You are an expert test engineer who excels in creating test cases\\\n",
    "    create test cases for the following code changes\\\n",
    "    Code:\\\n",
    "    {changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9254588",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=''\n",
    "for txt in resp.response_gen:\n",
    "    messages+=txt\n",
    "    Markdown(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(qe.query(f\"You are an expert test engineer who excels in creating test cases\\\n",
    "    create test cases for the following code changes\\\n",
    "    Code:\\\n",
    "    {changes}\").print_response_stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd672580",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response=\"\"\n",
    "for response in gen:\n",
    "    full_response += response.delta\n",
    "    time.sleep(0.01)\n",
    "    Markdown(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fe586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "Markdown(resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=qe.query(f\"\"\"\n",
    "        You are an expert test engineer who excels in creating test cases for IT products, \n",
    "        Analyze the following Python code and create test cases, Focusing on edge cases, input validation, and expected behavior.\n",
    "        \n",
    "        Code:\n",
    "        {changes}\n",
    "        \n",
    "        The test cases should follow the following format: \n",
    "        Test Case ID\tDescription\tPriority\tSteps\tExpected Result\tPass/Fail\tNotes\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qe=index.as_query_engine(streaming=True, similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329575fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=qe.query(f\"\"\"\n",
    "        You are an expert test engineer who excels in creating test cases for IT products, \n",
    "        Analyze the following Python code and create test cases, Focusing on edge cases, input validation, and expected behavior.\n",
    "        \n",
    "        Code:\n",
    "        {changes}\n",
    "        \n",
    "        The test cases should follow the following markdown format: \n",
    "\"| Test Case ID | TC-PERF-001 |\\n\",\n",
    "\"|--------------|------------|\\n\",\n",
    "\"| **Title** | Verify API response time under load |\\n\",\n",
    "\"| **Description** | Ensure GraphQL API responds within 500ms under expected load |\\n\",\n",
    "\"| **Preconditions** | Load testing environment set up |\\n\",\n",
    "\"| **Test Steps** | 1. Simulate concurrent users accessing API endpoints<br>2. Measure response times |\\n\",\n",
    "\"| **Expected Result** | API response times remain below 500ms under load |\\n\",\n",
    "\"\\n\",\n",
    "\"---\\n\",\n",
    "\"\\n\",\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a47253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "resp=\"\"\n",
    "\n",
    "for text in response.get_response:\n",
    "    resp+=text\n",
    "    Markdown(html(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=qe.a(f\"\"\"\n",
    "        You are an expert test engineer who excels in creating test cases for IT products, \n",
    "        Analyze the following Python code and create test cases, Focusing on edge cases, input validation, and expected behavior.\n",
    "        \n",
    "        Code:\n",
    "        {changes}\n",
    "        \n",
    "        The test cases should follow the following markdown format: \n",
    "\"| Test Case ID | TC-PERF-001 |\\n\",\n",
    "\"|--------------|------------|\\n\",\n",
    "\"| **Title** | Verify API response time under load |\\n\",\n",
    "\"| **Description** | Ensure GraphQL API responds within 500ms under expected load |\\n\",\n",
    "\"| **Preconditions** | Load testing environment set up |\\n\",\n",
    "\"| **Test Steps** | 1. Simulate concurrent users accessing API endpoints<br>2. Measure response times |\\n\",\n",
    "\"| **Expected Result** | API response times remain below 500ms under load |\\n\",\n",
    "\"\\n\",\n",
    "\"---\\n\",\n",
    "\"\\n\",\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2467b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48882759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
